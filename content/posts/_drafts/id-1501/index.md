---
title: "REWORK OR DELETE: No SQL: Load Balancing for Data Stores"
draft: true
---

If you’ve worked with distributed systems over the last decade you may concur that, whilst we’ve come a long way, it’s been a non-trivial affair. Take for example Component-Oriented Software, for example with EJB: One of the first distributed paradigms that many of us worked with – be it intentionally or not it has a tendency to abstract away the distributed characteristics of the system. Programing the distributed components becomes simpler and more accessible but at the risk of some potentially serious, performance-degrading side affects. Many of us learned we needed to be smart about the amount of data they move around and how frequently they choose to move it. By comparison Load Balancing is a very different beast. By splitting the work into self-contained units it has a tendency to push the programmer away from using (or needing) shared state, with each ‘balanced’ unit of processing tending to be discrete. In fact programming processing in many load balanced architectures often doesn’t even require cognisance of the distributed nature of the application as all the important business functionality is running in only one process at a time. So what should we learn from all this? Shared state certainly complicates any program’s architecture and the best policy may be to avoid it wherever possible. Where its implementation is unavoidable you need to be cognisant of the slower physical characteristics so that they can take appropriate action. Distribution needs to be embraced, not hidden away. NoSQL: Load-Balancing for data stores The NoSQL movement is in many ways load balancing for data storage. For simple problems, particularly in the OLTP space, data can be partitioned without shared state making the NoSQL solution is far more scalable than it’s traditional database brethren. NoSQL represents a change in priorities, with a range of technologies providing simple, scalable query mechanisms across physically partitioned data sets without the need for shared state. Partitioning (Sharding) really is load balancing for data stores. Conversely traditional databases will always struggle to embrace a truly distributed architecture. They cannot realise the same benefits from partitioning; they simply have too many contractual constraints that require a global view of state (locking, transactions etc). So is NoSQL the answer? It certainly works well in applications with simple, well-known query patterns (think OLTP), but the model breaks down when faced with the more complex use cases found in the enterprise space. This is because NoSQL hasn’t addressed the physical constraints of a distributed memory space - it has simply sidestepped the issue. So software engineers must determine whether they can compartmentalise their storage requirement in a way that avoids shared state? If they can then a world of scalable storage is at their fingertips. Fine for those willing to embrace the distributed world and understand the limitations of their technology choices. Less so for those who simply want a black box that ‘just works’. What portion of enterprise customers really fit into that former camp? The Brute Force Approach to Distribution Oracle has taken a different path, but to a similar destination. Unlike the NoSQL movement Oracle have looked to cutting edge hardware to relieve the performance constraints imposed by needing shared state. Their main offering, Exadata, is essentially a cluster of machines designed to perform like a single, large one. The engineering is second to none. The combination of infiniband networking and proprietary drivers for the network stack brings cross machine latencies into the ten’s of microseconds range (although this is a feature of their 1.1 release). Their combination of this with with the disk-head pre-filtering provided by <\*\*\*\*\*> presents a compelling architecture that has the potential to chew capably through most enterprise workloads with ease. The exatdata-like Oracle Supercluster recently smashed the latest TPC benchmarks at more than 30,000,000 transactions per minute, three times its closest competition. However, for all its temerity, it is still a brute force solution to the problem of shared state. The architectural pattern remains the same, simply accepts the reality of the problem and meet it head on. This is fundamentally different to the Shared Nothing approach, which embraces distribution. Oracle is in fact completely vacant in the ‘Shared Nothing’ space despite the pattern being a couple of decades old and enthusiastically touted by a host of alternative data storage products (Teradata, Vertica etc). This retained presence in the shared-disk space may be due the drawbacks of shared nothing architectures (discussed in a previous article \[Shared nothing vs Shared Disk – and independent view\]), most notably the fact that queries against keys other than the key used for partitioning, must always visit all nodes – a limit to scalability. However Oracle’s lack of presence in this space is is also likely, at least to some extent, to be due to their heritage. Database software spent several decades running on single machine architectures and moving to a segmented one is no simple task. Whilst a number of changes have been made to accommodate distribution, database products are still held back by their distant ancestors that evolved in a single-machine world. Michael Stonebreaker, a long-term evangelist from the lets-start-from-scratch camp argues: “\[the database\]… should be considered as legacy technology more than a quarter of a century in age, for which a complete redesign and re-architecting is the appropriate next step”. Michael’s point has merit. Any evolving technology will always be hampered, in some way or other, by the decisions of the past. The databases evolving from this camp have the majority of the properties of the traditional behemoths but with architectures that embrace distribution, providing significant gains in performance and scalability. The shared nothing approach embraces distribution at its heart but still pays the price for it. The Return of the In Memory Database

/\*\* not sure what to do with this: The advantages of knowing that all data is in memory shouldn’t be understated. It completely removes the need to mediate between RAM and disk. If all data is available in memory there is less need for intermediate results space as all data is freely accesible. \*\*/ To the insightful this question may appear somewhat naive. There are a variety of challenges that prohibit the use of completely in-process architectures, and they are the same challenges that have faced the in memory databases since their invention; the aforementioned issues of durability and that “one last bit”. But of course this progression is inevitable. In a decade or two the concept of holding our data on spinning magnetic disks will likely seem as arcane as plugging a cassette deck into a BBC micro does today. The interesting part is how we get there; the technological epochs we go through on the way. One thing is for sure, the IO based designs of traditional databases will find it increasing harder to compete with in memory alternatives. There are a few factors which will likely perpetuate this change: - Terabytes of RAM in commodity hardware. - Solid state disk provides storage that can be addressed directly at speeds close to that of RAM and very importantly is far more durable. Phase change memory closes the gap further with promises of greater latency, higher density and slower degradation. Having as single address space reduces the need to page data from disk to memory, making the solutions dramatically simpler and more efficient than traditional database architectures. - Users are more willing to give up certain traditional database features in exchange for dramatically radily trading consistency and analytical processing capabilities for scalability. - There is significant performance benefit to be found from engineering specifically for this new hardware platform. The major players are not oblivious to this. SAP and the Stonebreaker inspired VoltDB provided modern distributed in-memory solutions. These tehcnologies are still heavily immersed in the distributed space coming at loggerheads with the likes of Exadata. Oracle do have TimesTen, a solution engineered specifically for a single address space but it has yet to really take advantages of the onset of SSD. In summary there is potentially an opening in the enterprise space for in-memory solutions that utilise Solid State Disk or more probably Phase Change Memory, not as disk but an extension of the main memory space. In doing so such technologies should be able overcome the problems associated with existing in memory solutions without the need to be distributed. Such a data store could present several orders of magnitude of performance improviement over exisiting databases, in the enterprise space.

Durability is the simplest as it has been solved by a number of in-memory solutions through journaling. The problem of reconstituting data after a failure can be addressed as a separate concern to servicing user queries. The “one more bit” problem is more pervasive if you are running close to the memory limits of your system. Two key factors will likely mitigate this problem, the rapidly increasing capabilities of flash storage and the growing discrepancy between the amount of storage enterprise users need vs. the amount of DRAM that can be installed in commodity hardware The advantages of knowing that all data is in memory shouldn’t be understated. It completely removes the need to mediate data between RAM and disk. If all data is available in memory then the need for intermediate results, and the temporary spaces to compose them, is hugely reduced, as all data is freely available. This allure of pinned-memory architectures could be enough to provoke some clever solutions that open the pinned-memory database to a new market. The benefits of In-memory architectures are twofold. The latency benefits of memory are self explanatory but they can also employ much simpler architectures when compared to their disk-resident brethren, hugely decreasing the processing overhead of performing a user query. Like SAP, Oracle are rumoured to be working in this space. …

(use shore db example)
